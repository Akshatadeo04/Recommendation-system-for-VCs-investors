{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "key_mattermark = \"93284adac11d34633e1781d5fb0129f579741eeaa41daa2e1e7f553b60d16020\"\n",
    "key_crunchbase = \"6da4206c39f52b4f6bfd8ce2e83af25e\"\n",
    "API = 0\n",
    "null=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matterMarkData(key_mattermark):\n",
    "    '''\n",
    "    Mattermark is a website which provides REST API based funding event information.\n",
    "    However, it only provides 100 free API calls to access the data and after that\n",
    "    it the key will expire.However, we have downloaded the data and stored it in csv.\n",
    "\n",
    "    If API == 1, the data will be taken by REST API calls. It will generate the error\n",
    "    when free API calls are over.\n",
    "    If API == 0 : the data is already downloaded by website using the code \"API == 1\"\n",
    "    should be used,\n",
    "    '''\n",
    "    if API == 1:\n",
    "        mattermark_data = []\n",
    "        total = 50\n",
    "        for i in range(1,total):\n",
    "            response = requests.get(\"https://api.mattermark.com/fundings/?key=93284adac11d34633e1781d5fb0129f579741eeaa41daa2e1e7f553b60d16020&page=\"+str(i)+\"&per_page=10000\")\n",
    "                    \n",
    "            if response.status_code < 400:\n",
    "                fundingData = response.json()\n",
    "                for item in fundingData[\"fundings\"]:\n",
    "                    temp = {}\n",
    "                    temp[\"company_name\"] = item.get(\"company_name\", None)\n",
    "                    temp[\"company_category_code\"] = item.get(\"industry\", None)\n",
    "                    temp[\"company_country_code\"] = item.get(\"country\", None)\n",
    "                    temp[\"company_state_code\"] = item.get(\"state\", None)\t\n",
    "                    temp[\"company_region\"] = item.get(\"region\",None)\n",
    "                    temp[\"company_city\"] = item.get(\"city\", None)\n",
    "            \n",
    "                    #get funding round type and convert it to similar type as crunchbase data\n",
    "                    funding = item.get(\"series\", None)\n",
    "                    if funding == 'a':\n",
    "                        temp[\"funding_round_type\"] = \"series-a\"\n",
    "                    elif funding == 'b':\n",
    "                        temp[\"funding_round_type\"] = \"series-b\"\n",
    "                    elif funding == 'c' or funding == 'd':\n",
    "                        temp[\"funding_round_type\"] = \"series-c+\"\n",
    "                    elif funding == \"unknown\":\n",
    "                        temp[\"funding_round_type\"] = \"other\"\n",
    "                    else:\n",
    "                        temp[\"funding_round_type\"] = \"series-c+\"\n",
    "\n",
    "                    #get funding dates and convert it to similar type as crunchbase data\n",
    "                    temp[\"funded_at\"] = item.get(\"rounds_funding_date\", None)\n",
    "                    x = item.get(\"rounds_funding_date\", None)\n",
    "                    if x == None:\n",
    "                        temp[\"funded_year\"] = None\n",
    "                    else:\n",
    "                        temp[\"funded_year\"] = x.split('-')[0]\n",
    "                    temp[\"raised_amount_usd\"] = item.get(\"amount\", None)\n",
    "\n",
    "                    #get investors data and split if there are multiple investors\n",
    "                    investor = item.get(\"investors\", None).split(\",\")\n",
    "                    for item in investor:\n",
    "                        temp[\"investor_name\"] = investor\n",
    "\n",
    "                        mattermark_data.append(temp)\n",
    "                mattermark_dataset = pd.DataFrame(mattermark_data)\n",
    "                mattermark_dataset.to_csv(\"C:/Akshata/Courses/summer19/large_scale_analytics/group_project/mattermark.csv\")\n",
    "            else:\n",
    "                print(\"Error Invalid response: \", response.status_code)\n",
    "                mattermark_dataset = pd.DataFrame(mattermark_data)\n",
    "                mattermark_dataset.to_csv(\"C:/Akshata/Courses/summer19/large_scale_analytics/group_project/mattermark.csv\")\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        mattermark_dataset = pd.read_csv(\"C:/Akshata/Courses/summer19/large_scale_analytics/group_project/mattermark.csv\", encoding='unicode_escape', dtype=object)\n",
    "    mattermark_dataset.drop(mattermark_dataset.columns[0],axis =1, inplace=True)\n",
    "    return mattermark_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crunchBaseData(key_crunchbase):\n",
    "    #The key in this code is already expired. The data is already saved as a csv file locally and will be used.\n",
    "    '''\n",
    "    response = requests.get('https://api.crunchbase.com/bulk/v4/bulk_export.tar.gz?user_key=6da4206c39f52b4f6bfd8ce2e83af25e')\n",
    "    if response.status_code < 400:\n",
    "        crunchbase_dataset = pd.read_csv('bulk_export.tar.gz', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n",
    "    else:\n",
    "        print(\"Error Invalid response:\", response.status_code)\n",
    "    '''\n",
    "    #get crunchbase dataset\n",
    "    crunchbase_dataset = pd.read_csv(\"C:/Akshata/Courses/summer19/large_scale_analytics/group_project/crunchbase-investments.csv\", encoding='unicode_escape', dtype=object)\n",
    "\n",
    "    crunchbase_dataset=crunchbase_dataset.drop(['company_permalink', 'investor_permalink', 'investor_category_code', 'investor_country_code', 'investor_state_code','investor_region', 'investor_city','funded_month', 'funded_quarter'],axis=1)\n",
    "\n",
    "    crunchbase_dataset.duplicated(subset=None, keep='first')\n",
    "    return crunchbase_dataset\n",
    "\n",
    "\n",
    "def dataPreprocessing(crunchbase_dataset, mattermark_dataset):\n",
    "    #join datasets\n",
    "    total_dataset = pd.concat([crunchbase_dataset, mattermark_dataset], sort=True)\n",
    "\n",
    "    #preprocess dataset\n",
    "    total_dataset.dropna(axis=0, subset=['company_name','investor_name', 'raised_amount_usd',])\n",
    "    total_dataset.dropna(inplace=True)\n",
    "\n",
    "    #divide test data and training data\n",
    "    msk = np.random.rand(len(total_dataset)) < 0.8\n",
    "    df_train = total_dataset[msk]\n",
    "    df_test = total_dataset[~msk]\n",
    "    \n",
    "    #label encoding\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le_ = preprocessing.LabelEncoder()\n",
    "\n",
    "    #label encoding for training data\n",
    "    df_train.company_category_code = le.fit_transform(df_train.company_category_code)\n",
    "    df_train.company_country_code = le.fit_transform(df_train.company_country_code)\n",
    "    df_train.company_state_code = le.fit_transform(df_train.company_state_code)\n",
    "    df_train.funding_round_type = le.fit_transform(df_train.funding_round_type)\n",
    "    df_train.raised_amount_usd = le.fit_transform(df_train.raised_amount_usd)\n",
    "\n",
    "    #label encoding for test data\n",
    "    df_test.company_category_code = le_.fit_transform(df_test.company_category_code)\n",
    "    df_test.company_country_code = le_.fit_transform(df_test.company_country_code)\n",
    "    df_test.company_state_code = le_.fit_transform(df_test.company_state_code)\n",
    "    df_test.funding_round_type = le_.fit_transform(df_test.funding_round_type)\n",
    "    df_test.raised_amount_usd = le_.fit_transform(df_test.raised_amount_usd)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "    \n",
    "def collaborativeFiltering(df_train, df_test):\n",
    "    '''\n",
    "    collaborative filtering is performed on investor basis.    \n",
    "    '''\n",
    "    #get unique investors in the dataset\n",
    "    i=0\n",
    "    similarity=0\n",
    "    sim=0\n",
    "    test_data=0\n",
    "    train_data=0\n",
    "    recommender = {}\n",
    "    for test_name in df_test.investor_name.unique():\n",
    "        df_test_current = df_test[df_test['investor_name']==test_name]\n",
    "        for name in df_train.investor_name.unique():\n",
    "            df_train_current = df_train[df_train['investor_name'] == name]\n",
    "            for test_index, test_item in df_test_current.iterrows():\n",
    "                for train_index, train_item in df_train_current.iterrows():\n",
    "                    x = [train_item['company_category_code'],train_item['company_country_code'], train_item['company_state_code'], train_item['funding_round_type'], train_item['raised_amount_usd']]\n",
    "                    x = np.array(x).reshape(1,-1)\n",
    "                    y = [test_item['company_category_code'],test_item['company_country_code'], test_item['company_state_code'], test_item['funding_round_type'], test_item['raised_amount_usd']]\n",
    "                    y = np.array(y).reshape(1,-1)\n",
    "                    matrix = cosine_similarity(x,y)\n",
    "                    sim+=matrix[0][0]\n",
    "                #Moving average calculation\n",
    "                sim/=(len(df_train_current))\n",
    "                similarity+=sim\n",
    "                sim=0\n",
    "            #moving average calculation\n",
    "            similarity/=(len(df_test_current))\n",
    "            #Add final simialrity of test investor with train investor to the final dictionary            \n",
    "            if recommender.get(test_name, None):\n",
    "                recommender[test_name].update({name:similarity})\n",
    "            else: recommender[test_name] = {name:similarity}\n",
    "            similarity=0\n",
    "        match = heapq.nlargest(10, recommender[test_name].items(), key=lambda i:i[1])\n",
    "\n",
    "        #provide recommendation to the investor\n",
    "        i=0\n",
    "        print(\"Recommendation for \",test_name,\" based on Collaborative Filtering:- \")\n",
    "        companies = []\n",
    "        for item in match:\n",
    "            if i < 10:\n",
    "                x = df_train.loc[df_train['investor_name'] == item[0]]['company_name'].values[0]\n",
    "                if x in companies:\n",
    "                    pass\n",
    "                else:\n",
    "                    companies.append(x)\n",
    "                    i+=1\n",
    "            else: break\n",
    "        for x in companies:\n",
    "            print(\"    \", x)\n",
    "\n",
    "        #Break statement is inserted here to come out of the loop after providing recommendation only for one test investor.\n",
    "        #Remove break if recommendation is required for more than one test investor\n",
    "        break\n",
    "    return 0\n",
    "\n",
    "    \n",
    "def contentBasedFiltering(df_train, df_test):\n",
    "    '''\n",
    "    content based filtering is performed on per company basis.    \n",
    "    '''\n",
    "    \n",
    "    #get unique investors in the dataset\n",
    "    i=0\n",
    "    similarity=0\n",
    "    sim=0\n",
    "    test_data=0\n",
    "    train_data=0\n",
    "    recommender = {}\n",
    "    for test_name in df_test.company_name.unique():\n",
    "        df_test_current = df_test[df_test['company_name']==test_name]\n",
    "        for name in df_train.company_name.unique():\n",
    "            df_train_current = df_train[df_train['company_name'] == name]\n",
    "            for test_index, test_item in df_test_current.iterrows():\n",
    "                for train_index, train_item in df_train_current.iterrows():\n",
    "                    x = [train_item['company_category_code'],train_item['company_country_code'], train_item['company_state_code'], train_item['funding_round_type'], train_item['raised_amount_usd']]\n",
    "                    x = np.array(x).reshape(1,-1)\n",
    "                    y = [test_item['company_category_code'],test_item['company_country_code'], test_item['company_state_code'], test_item['funding_round_type'], test_item['raised_amount_usd']]\n",
    "                    y = np.array(y).reshape(1,-1)\n",
    "                    matrix = cosine_similarity(x,y)\n",
    "                    sim+=matrix[0][0]\n",
    "                #Moving average calculation\n",
    "                sim/=(len(df_train_current))\n",
    "                similarity+=sim\n",
    "                sim=0\n",
    "            #moving average calculation\n",
    "            similarity/=(len(df_test_current))\n",
    "            #Add final simialrity of test investor with train investor to the final dictionary            \n",
    "            if recommender.get(test_name, None):\n",
    "                recommender[test_name].update({name:similarity})\n",
    "            else: recommender[test_name] = {name:similarity}\n",
    "            similarity=0\n",
    "        match = heapq.nlargest(10, recommender[test_name].items(), key=lambda i:i[1])\n",
    "        #provide recommendation to the investor\n",
    "        i=0\n",
    "        print(\"Recommendation for \",df_test_current['investor_name'].item(),\" based on Content Based Filtering:-\")\n",
    "        for item in match:\n",
    "            print(\"    \", item[0])\n",
    "        #Break statement is inserted here to come out of the loop after providing recommendation only using one test company.\n",
    "        #Remove break if recommendation is required using more than one test company.\n",
    "        break\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mattermark_dataset = matterMarkData(key_mattermark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunchbase_dataset = crunchBaseData(key_crunchbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = dataPreprocessing(crunchbase_dataset, mattermark_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation for  10Xelerator  based on Content Based Filtering:-\n",
      "     Oncofactor Corporation\n",
      "     General Dynamics\n",
      "     TappIn\n",
      "     SnowShoe Stamp\n",
      "     SplitSecnd\n",
      "     CoverWallet\n",
      "     AppTrigger\n",
      "     ID.me\n",
      "     Aclaris Therapeutics\n",
      "     TriActive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentBasedFiltering(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation for  10Xelerator  based on Collaborative Filtering:- \n",
      "     Moda Operandi\n",
      "     Oncofactor Corporation\n",
      "     Geospock\n",
      "     CoverWallet\n",
      "     ID.me\n",
      "     Swept\n",
      "     Plextronics\n",
      "     MetaSolv\n",
      "     Mental Canvas\n",
      "     Palyon Medical\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborativeFiltering(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
